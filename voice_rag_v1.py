# -*- coding: utf-8 -*-
"""VOICE-RAG v1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11j59jHPuWXHGuxIjrtJEHmsPjZFQpybp

```
# Voice RAG System - Simple POC
```
"""

# Cell 1: Check GPU and install compatible packages for T4
import torch
print(f"🔥 GPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("⚠️ No GPU detected. Switch to T4 GPU runtime!")

# Install packages with compatible versions
import subprocess
import sys

def install_packages():
    packages = [
        "transformers>=4.41.0",
        "torch",
        "torchaudio",
        "accelerate",
        "bitsandbytes",
        "optimum",
        "requests",
        "beautifulsoup4",
        "gradio",
        "sentence-transformers",
        "datasets",
        "librosa",
        "soundfile"
    ]

    # subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade"] + packages)
    subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade"] + packages)

install_packages()

def install_faiss_gpu():
    try:
        # Try installing faiss-gpu through conda-forge
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "faiss-gpu"])
        print("✅ faiss-gpu installed successfully")
        return True
    except:
        try:
            # Fallback: install faiss-cpu and use GPU for embeddings only
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "faiss-cpu"])
            print("⚠️ Using faiss-cpu (embeddings still GPU-accelerated)")
            return False
        except:
            print("❌ FAISS installation failed, using basic similarity search")
            return None

faiss_gpu_available = install_faiss_gpu()

import torch
import torchaudio
from transformers import (
    AutoModelForSpeechSeq2Seq,
    AutoProcessor,
    pipeline,
    BitsAndBytesConfig
)
import requests
from bs4 import BeautifulSoup
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import gradio as gr
from typing import List, Dict
import warnings
import gc
import librosa
import soundfile as sf
from datetime import datetime
warnings.filterwarnings("ignore")

# Try to import FAISS (handle different installation scenarios)
try:
    import faiss
    FAISS_AVAILABLE = True
    print("✅ FAISS imported successfully")
except ImportError:
    FAISS_AVAILABLE = False
    print("⚠️ FAISS not available, using sklearn for similarity")
    from sklearn.metrics.pairwise import cosine_similarity

# Set optimal settings for T4
torch.backends.cudnn.benchmark = True
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True

# GPU-Optimized Voice RAG System
class VoiceRAGT4:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"🚀 Using device: {self.device}")

        # Initialize models
        self.speech_to_text = None
        self.processor = None
        self.embedder = None

        # Setup models with GPU optimization
        self.setup_models()

        # Document store
        self.documents = []
        self.document_embeddings = None
        self.faiss_index = None

    def setup_models(self):
        """Setup models optimized for T4 GPU"""
        print("🔧 Loading models with T4 optimizations...")

        # Setup Speech-to-Text with your model
        try:
            model_id = "AfroLogicInsect/whisper-finetuned-float32"

            # Use BitsAndBytesConfig for memory optimization
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,  # Reduces memory usage
                bnb_8bit_compute_dtype=torch.float16
            )

            # Load model with optimizations
            model = AutoModelForSpeechSeq2Seq.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                use_safetensors=True,
                quantization_config=bnb_config if self.device == "cuda" else None
            )

            if self.device == "cuda":
                model = model.to("cuda")

            processor = AutoProcessor.from_pretrained(model_id)

            # Create optimized pipeline
            self.speech_to_text = pipeline(
                "automatic-speech-recognition",
                model=model,
                tokenizer=processor.tokenizer,
                feature_extractor=processor.feature_extractor,
                max_new_tokens=128,
                chunk_length_s=30,  # Process in chunks for memory efficiency
                batch_size=8,       # Optimal batch size for T4
                torch_dtype=torch.float16,
                device=0 if self.device == "cuda" else -1,
            )

            print("✅ Your Whisper model loaded successfully with T4 optimizations")

        except Exception as e:
            print(f"❌ Error loading your model: {e}")
            print("🔄 Falling back to optimized Whisper-small")

            # Fallback with optimizations
            self.speech_to_text = pipeline(
                "automatic-speech-recognition",
                model="openai/whisper-small",
                torch_dtype=torch.float16,
                device=0 if self.device == "cuda" else -1,
                chunk_length_s=30,
                batch_size=8
            )

        # Setup embedding model with GPU
        print("🧠 Loading sentence transformer with GPU...")
        self.embedder = SentenceTransformer(
            'all-MiniLM-L6-v2',
            device=self.device
        )

        # Optimize embedder for inference
        if self.device == "cuda":
            self.embedder.half()  # Use half precision for speed

        print("✅ All models loaded and optimized for T4 GPU")

    def preprocess_audio(self, audio_path):
        """Preprocess audio for optimal Whisper performance"""
        try:
            # Load audio with librosa (handles various formats better)
            audio, sr = librosa.load(audio_path, sr=16000)  # Whisper expects 16kHz

            # Normalize audio
            audio = librosa.util.normalize(audio)

            # Apply noise reduction (simple)
            audio = librosa.effects.preemphasis(audio)

            return audio
        except Exception as e:
            print(f"Audio preprocessing error: {e}")
            return None

    def transcribe_audio(self, audio_path):
        """GPU-accelerated speech-to-text with optimizations"""
        try:
            # Preprocess audio
            audio = self.preprocess_audio(audio_path)
            if audio is None:
                return "Error: Could not process audio file"

            # Clear GPU cache before inference
            if self.device == "cuda":
                torch.cuda.empty_cache()

            # Transcribe with optimized settings
            with torch.cuda.amp.autocast():  # Mixed precision for speed
                result = self.speech_to_text(
                    audio,
                    generate_kwargs={
                        "max_new_tokens": 128,
                        "num_beams": 2,  # Reduced for speed
                        "do_sample": False,
                        "temperature": 1.0,
                        "use_cache": True
                    }
                )

            # Clear cache after inference
            if self.device == "cuda":
                torch.cuda.empty_cache()

            return result["text"].strip()

        except Exception as e:
            print(f"Transcription error: {e}")
            return f"Error in transcription: {e}"

    def web_search(self, query: str, num_results: int = 5) -> List[Dict]:
        """Fast web search with caching"""
        try:
            # Use DuckDuckGo instant answer API
            url = f"https://api.duckduckgo.com/?q={query}&format=json&no_html=1&skip_disambig=1"
            response = requests.get(url, timeout=5)  # Reduced timeout for speed
            data = response.json()

            results = []

            # Process results efficiently
            if data.get('Abstract'):
                results.append({
                    'title': data.get('AbstractSource', 'DuckDuckGo')[:50],
                    'content': data['Abstract'][:300],  # Limit content length
                    'url': data.get('AbstractURL', ''),
                    'relevance': 1.0
                })

            # Get related topics
            for topic in data.get('RelatedTopics', [])[:num_results-1]:
                if isinstance(topic, dict) and topic.get('Text'):
                    results.append({
                        'title': (topic.get('FirstURL', '').split('/')[-1] or 'Related')[:50],
                        'content': topic['Text'][:300],
                        'url': topic.get('FirstURL', ''),
                        'relevance': 0.8
                    })

            return results

        except Exception as e:
            return [{'title': 'Search Error', 'content': f'Search failed: {e}', 'url': '', 'relevance': 0}]

    def add_documents_batch(self, documents: List[str], batch_size: int = 32):
        """GPU-accelerated document embedding in batches with fallback options"""
        print(f"📚 Processing {len(documents)} documents in batches of {batch_size}...")

        self.documents.extend(documents)

        # Clear GPU memory
        if self.device == "cuda":
            torch.cuda.empty_cache()

        # Process in batches for memory efficiency
        all_embeddings = []
        for i in range(0, len(self.documents), batch_size):
            batch = self.documents[i:i+batch_size]

            if self.device == "cuda":
                with torch.cuda.amp.autocast():
                    batch_embeddings = self.embedder.encode(
                        batch,
                        batch_size=batch_size,
                        show_progress_bar=True,
                        convert_to_numpy=True,
                        normalize_embeddings=True
                    )
            else:
                batch_embeddings = self.embedder.encode(
                    batch,
                    batch_size=batch_size,
                    show_progress_bar=True,
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )

            all_embeddings.append(batch_embeddings)

            # Clear cache between batches
            if self.device == "cuda":
                torch.cuda.empty_cache()

        # Combine all embeddings
        self.document_embeddings = np.vstack(all_embeddings)

        # Create search index
        if FAISS_AVAILABLE:
            dimension = self.document_embeddings.shape[1]

            # Try GPU FAISS first, fallback to CPU FAISS
            try:
                if self.device == "cuda":
                    res = faiss.StandardGpuResources()
                    self.faiss_index = faiss.GpuIndexFlatIP(res, dimension)
                    print("🚀 Using GPU-accelerated FAISS")
                else:
                    self.faiss_index = faiss.IndexFlatIP(dimension)
                    print("🔧 Using CPU FAISS")
            except Exception as e:
                print(f"⚠️ GPU FAISS failed, using CPU FAISS: {e}")
                self.faiss_index = faiss.IndexFlatIP(dimension)

            # Add embeddings to index
            self.faiss_index.add(self.document_embeddings.astype(np.float32))
        else:
            # Use sklearn as fallback
            print("🔧 Using sklearn cosine similarity (no FAISS)")
            self.faiss_index = None

        print(f"✅ Added {len(documents)} documents with T4 acceleration")

    def retrieve_documents_fast(self, query: str, k: int = 5) -> List[Dict]:
        """GPU-accelerated document retrieval with fallback options"""
        if len(self.documents) == 0:
            return []

        try:
            # Clear GPU cache
            if self.device == "cuda":
                torch.cuda.empty_cache()

            # Encode query with GPU
            if self.device == "cuda":
                with torch.cuda.amp.autocast():
                    query_embedding = self.embedder.encode(
                        [query],
                        normalize_embeddings=True,
                        convert_to_numpy=True
                    )
            else:
                query_embedding = self.embedder.encode(
                    [query],
                    normalize_embeddings=True,
                    convert_to_numpy=True
                )

            results = []

            if FAISS_AVAILABLE and self.faiss_index is not None:
                # Use FAISS for fast search
                scores, indices = self.faiss_index.search(
                    query_embedding.astype(np.float32),
                    min(k, len(self.documents))
                )

                for i, score in zip(indices[0], scores[0]):
                    if score > 0.25:  # Similarity threshold
                        results.append({
                            'content': self.documents[i],
                            'score': float(score),
                            'index': int(i)
                        })
            else:
                # Fallback: use sklearn cosine similarity
                if hasattr(self, 'document_embeddings') and self.document_embeddings is not None:
                    similarities = cosine_similarity(query_embedding, self.document_embeddings)[0]

                    # Get top k similar documents
                    top_indices = np.argsort(similarities)[::-1][:k]

                    for idx in top_indices:
                        score = similarities[idx]
                        if score > 0.25:
                            results.append({
                                'content': self.documents[idx],
                                'score': float(score),
                                'index': int(idx)
                            })

            return results

        except Exception as e:
            print(f"Retrieval error: {e}")
            return []

    def generate_response_optimized(self, query: str, search_results: List[Dict], retrieved_docs: List[Dict]) -> str:
        """Generate optimized response with relevance scoring"""

        context_parts = []

        # Add retrieved documents (sorted by relevance)
        if retrieved_docs:
            context_parts.append("📚 From Knowledge Base (most relevant):")
            retrieved_docs.sort(key=lambda x: x['score'], reverse=True)
            for doc in retrieved_docs[:3]:  # Top 3 most relevant
                context_parts.append(f"• {doc['content'][:200]}... (relevance: {doc['score']:.2f})")

        # Add search results
        if search_results:
            context_parts.append("\n🌐 From Web Search:")
            for result in search_results[:3]:
                if result['content']:
                    context_parts.append(f"• {result['title']}: {result['content'][:150]}...")

        context = "\n".join(context_parts)

        if not context.strip():
            return "❌ I couldn't find relevant information to answer your question. Try rephrasing or asking about a different topic."

        # Enhanced response format
        response = f"""🎯 **Query**: {query}

{context}

💡 **Summary**: Based on the retrieved information above, this covers the key aspects of your question from both the knowledge base and current web sources."""

        return response

    def process_voice_query_optimized(self, audio_file):
        """Optimized main pipeline with performance monitoring"""
        start_time = datetime.now()

        try:
            # Step 1: Speech to Text (GPU accelerated)
            print("🎤 Transcribing audio with T4 acceleration...")
            stt_start = datetime.now()
            text_query = self.transcribe_audio(audio_file)
            stt_time = (datetime.now() - stt_start).total_seconds()
            print(f"📝 Transcribed in {stt_time:.2f}s: {text_query}")

            if text_query.startswith("Error"):
                return text_query, "", f"Transcription failed in {stt_time:.2f}s"

            # Step 2: Parallel retrieval and search
            print("🔍 Performing parallel search and retrieval...")
            search_start = datetime.now()

            # GPU-accelerated document retrieval
            retrieved_docs = self.retrieve_documents_fast(text_query, k=5)

            # Web search (can run in parallel)
            search_results = self.web_search(text_query, num_results=3)

            search_time = (datetime.now() - search_start).total_seconds()

            # Step 3: Generate response
            print("💭 Generating optimized response...")
            response_start = datetime.now()
            response = self.generate_response_optimized(text_query, search_results, retrieved_docs)
            response_time = (datetime.now() - response_start).total_seconds()

            total_time = (datetime.now() - start_time).total_seconds()

            # Performance summary
            perf_summary = f"""⚡ Performance (T4 GPU):
• Transcription: {stt_time:.2f}s
• Search & Retrieval: {search_time:.2f}s
• Response Generation: {response_time:.2f}s
• Total Time: {total_time:.2f}s
• Retrieved Docs: {len(retrieved_docs)}
• Search Results: {len(search_results)}"""

            return text_query, response, perf_summary

        except Exception as e:
            error_time = (datetime.now() - start_time).total_seconds()
            return f"❌ System Error: {e}", "", f"Failed after {error_time:.2f}s"

    def clear_gpu_memory(self):
        """Clear GPU memory to prevent OOM"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            gc.collect()
            print("🧹 GPU memory cleared")

# Initialize T4-optimized system
print("🚀 Initializing T4-optimized Voice RAG system...")
voice_rag = VoiceRAGT4()


# Add sample documents optimized for batch processing
print("📚 Adding knowledge base documents...")
ai_docs = [
    "Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.",
    "Machine Learning is a subset of AI that provides systems the ability to automatically learn and improve from experience.",
    "Deep Learning is a machine learning technique that teaches computers to learn by example, using neural networks.",
    "Natural Language Processing (NLP) helps computers understand, interpret and generate human language in a valuable way.",
    "Computer Vision enables machines to identify and analyze visual content in images and videos.",
    "Neural networks are computing systems inspired by biological neural networks that constitute animal brains.",
    "Large Language Models (LLMs) are AI models trained on vast amounts of text data to understand and generate human language.",
    "Transformer architecture is the foundation of modern language models, using attention mechanisms.",
    "GPU acceleration significantly speeds up AI model training and inference through parallel processing.",
    "Fine-tuning allows pre-trained models to be adapted for specific tasks with smaller datasets."
]

voice_rag.add_documents_batch(ai_docs, batch_size=16)

def process_audio_interface_t4(audio):
    """T4-optimized interface function"""
    if audio is None:
        return "Please upload an audio file", "", "No audio provided"

    # Process with optimizations
    result = voice_rag.process_voice_query_optimized(audio)

    # Clear GPU memory after processing
    voice_rag.clear_gpu_memory()

    return result

# Enhanced Gradio interface
interface = gr.Interface(
    fn=process_audio_interface_t4,
    inputs=gr.Audio(
        type="filepath",
        label="🎤 Record or Upload Audio",
        sources=["microphone", "upload"]
    ),
    outputs=[
        gr.Textbox(label="📝 Transcribed Text", lines=3, max_lines=5),
        gr.Textbox(label="🤖 AI Response", lines=12, max_lines=20),
        gr.Textbox(label="⚡ Performance Metrics", lines=8, max_lines=10)
    ],
    title="🚀 Voice RAG System - T4 GPU Optimized",
    description="""
    **GPU-Accelerated Voice Question-Answering System**

    Upload audio or record your voice to ask questions. The system uses:
    • Your fine-tuned Whisper model for speech recognition
    • GPU-accelerated semantic search
    • Real-time web search
    • Optimized for T4 GPU runtime

    Try asking about: AI, machine learning, technology, or any general topic!
    """,
    theme=gr.themes.Soft(),
    allow_flagging="never"
)

# Launch optimized interface
if __name__ == "__main__":
    print("🎉 Launching T4-optimized Voice RAG interface...")
    interface.launch(
        share=True,
        debug=True,
        server_port=7860,
        show_error=True
    )

# Performance testing and utilities
def benchmark_system():
    """Benchmark the T4-optimized system"""
    test_queries = [
        "What is machine learning?",
        "How does deep learning work?",
        "Explain artificial intelligence",
        "What are neural networks?",
        "How do transformers work in AI?"
    ]

    print("🏁 Running T4 GPU benchmark...")
    total_times = []

    for i, query in enumerate(test_queries):
        print(f"\nTest {i+1}/5: {query}")
        start = datetime.now()

        # Simulate the pipeline without audio
        retrieved = voice_rag.retrieve_documents_fast(query, k=3)
        search_results = voice_rag.web_search(query, num_results=3)
        response = voice_rag.generate_response_optimized(query, search_results, retrieved)

        elapsed = (datetime.now() - start).total_seconds()
        total_times.append(elapsed)
        print(f"⏱️ Completed in {elapsed:.2f}s")

        voice_rag.clear_gpu_memory()

    avg_time = np.mean(total_times)
    print(f"\n📊 Benchmark Results:")
    print(f"Average query time: {avg_time:.2f}s")
    print(f"Queries per minute: {60/avg_time:.1f}")
    print(f"GPU utilization: Optimized for T4")

def add_custom_documents_batch():
    """Add your custom documents efficiently"""
    custom_docs = [
        # Replace with your domain-specific documents
        "Add your custom knowledge base content here",
        "Domain-specific information goes in this list",
        "Each string is a separate document",
        # Add more documents...
    ]

    if custom_docs[0] != "Add your custom knowledge base content here":
        voice_rag.add_documents_batch(custom_docs, batch_size=16)
        print(f"✅ Added {len(custom_docs)} custom documents with T4 acceleration")
    else:
        print("💡 Edit this cell to add your custom documents")

# Uncomment to run benchmark
benchmark_system()

# Uncomment to add custom documents
# add_custom_documents_batch()

print("\n🎉 T4-Optimized Voice RAG System Ready!")
print("📈 Performance features:")
print("  • GPU-accelerated speech recognition")
print("  • Fast semantic search with GPU FAISS")
print("  • Mixed precision inference")
print("  • Memory optimization for T4")
print("  • Batch processing for efficiency")
print("\n🚀 Launch the Gradio interface to start testing!")